<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head><meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    .hp-photo{ width:240px; height:240px; border-radius:240px; -webkit-border-radius:240px; -moz-border-radius:240px; }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 24px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    </style>

    <title>Aishan Liu | Beihang University</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
    <link rel="icon" type="image/jpg" href="./imgs/buaa_icon.jpg">
</head>

<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td>


    <!--SECTION 1 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center"><name>Aishan Liu (刘艾杉)</name></p>
                  <p align="justify">I am a 4th Year Ph.D. candidate student (2017.09-) in the <a href="http://www.nlsde.buaa.edu.cn/">National State Key Laboratory of Software Development Environment</a>
                    at <a href="https://www.buaa.edu.cn/">Beihang University</a>, supervised by Profs. <a href="http://sites.nlsde.buaa.edu.cn/~liwei/">Wei Li</a> 
                    and Profs. <a href="http://sites.nlsde.buaa.edu.cn/~xlliu/">Xianglong Liu</a>.
                    Before that, I obtained my M.Sc and B.Sc degree from <a href="https://www.buaa.edu.cn/">Beihang University</a> at 2016 and 2013, respectively, 
					where I was supervised by Profs. <a href="http://sites.nlsde.buaa.edu.cn/~liwei/">Wei Li</a>.

                    </br></br>
                    In my Ph.D study, during 2021, I was a visiting student at <a href="https://www.berkeley.edu/">UC Berkeley</a>, supervised by Profs. <a href="http://people.eecs.berkeley.edu/~dawnsong/">Dawn Song</a>.
					During 2020, I was a visiting student at <a href="https://www.sydney.edu.au/">the University of Sydney</a>, supervised by Profs. <a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/dacheng-tao.html">Dacheng Tao</a>. 
					In 2019, I interned at <a href="https://ai.tencent.com/ailab/zh/index">AI Lab</a> at <a href="https://www.tencent.com/">Tencent</a>, supervised by Profs. <a href="https://lwwangcse.github.io/">Liwei Wang</a>.

					<br><br><strong>Email:</strong> liuaishan AT buaa DOT edu DOT cn
	            </br>
                </p><p align="center">
                    <a href="https://scholar.google.com/citations?user=88tzr_sAAAAJ&hl=zh-CN&oi=ao">Google Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/liuaishan"> Github </a> &nbsp;/&nbsp;
					<a href="https://dblp.org/pid/177/5658.html"> dblp </a>
                </p>
              </td>
			  <td align="right"> <img class="hp-photo" src="./imgs/photo.jpg" style="width: 165;"></td></tr>
            </tbody>
          </table>

    <!--SECTION 2 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>Research</heading>
		  <p align="justify">My research interests include some sub-fields of <b>Computer Vision</b> and <b>Deep Learning</b>:</p>
<ul>
<li><p><b>Robust Deep Neural Networks</b>: Adversarial Example, Model Robustness, AI Safety</p>
</li>
</ul>
</p></br>
     
       </tbody>
    </table>

    <!--SECTION 3 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td><heading>News</heading>
		    <p style="font-size:13px"> <strong><font color="red">[Call for Papers]</font></strong> I am co-organizing the Workshop on <a href="https://advm-workshop-2021.github.io/">1st International Workshop on Adversarial Learning for Multimedia</a> at ACM MM 2021. 
			Please submit your papers and win the prizes!</p>

            <p style="font-size:13px"> <strong>[2021.03]</strong>  One paper accepted by CVPR 2021 (Oral).</p>

            <p style="font-size:13px"> <strong>[2021.02]</strong>  I am serving as a Senior Program Committee for IJCAI 2021.</p>

            <p style="font-size:13px"> <strong>[2020.12]</strong>  Our open-source platform 重明 has been awarded the <a href="http://www.techweb.com.cn/2020-12-02/2814466.shtml">首届OpenI启智社区优秀开源项目</a> (First OpenI Excellent Open Source Project).</p>

            <p style="font-size:13px"> <strong>[2020.11]</strong> One paper accepted for IEEE TIP.</p>

            <p style="font-size:13px"> <strong>[2020.07]</strong> Two papers accepted for ECCV 2020.</p>
          </td>
       </tr></tbody>
    </table>

    <!--SECTION 4 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td><heading>Publications</heading>
          </td>
          </tr>
		  </tbody>
    </table>

	<h3>&nbsp;&nbsp;&nbsp;&nbsp;Conference Papers</h3>
    <!--SECTION 5 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody><tr>

   <td width="20%"><img src="./imgs/CVPR2021.png" alt="CVPR2021" width="180" height = "110" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://arxiv.org/pdf/2103.01050.pdf">
                 <papertitle>Dual Attention Suppression Attack: Generate Adversarial Camouflage in Physical World</papertitle></a>
                 <br>Jiakai Wang, <strong>Aishan Liu</strong>, Zixin Yin, Shunchang Liu, Shiyu Tang, Xianglong Liu.<br>
                 <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021
				 <font color="red"><strong>(Oral)</strong></font>
                 <br>
                 <a href="https://arxiv.org/pdf/2103.01050.pdf">pdf</a> /
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s/cIcJvmkbvQk-W_qJADkSqw"><font color="red">(机器之心)</font></a>
                /<a href="https://github.com/liuaishan/SpatiotemporalAttack"><font color="red">Project page</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=nlsde-safety-team&repo=DualAttentionAttack&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
				
                 <p align="justify" style="font-size:13px">We propose the Dual Attention Suppression (DAS) attack to generate visually-natural physical adversarial camouflages with strong transferability by suppressing both model and human attention. </p>
                <p></p>
            </td>
        </tr>
		

   <td width="20%"><img src="./imgs/ECCV_1.png" alt="eccv2020" width="180" height = "110" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://arxiv.org/pdf/2005.09161.pdf">
                 <papertitle>Spatiotemporal Attacks for Embodied Agents</papertitle></a>
                 <br><strong>Aishan Liu</strong>, Tairan Huang, Xianglong Liu, Yitao Xu, Yuqing Ma, Xinyun Chen, Stephen Maybank, Dacheng Tao.<br>
                 <em>European Conference on Computer Vision (ECCV)</em>, 2020
                 <br>
                 <a href="https://arxiv.org/pdf/2005.09161.pdf">pdf</a> /
                <font color="red"> News:</font>
                <a href="https://www.qbitai.com/2020/07/16562.html"><font color="red">(量子位)</font></a>
                /<a href="https://github.com/liuaishan/SpatiotemporalAttack"><font color="red">Project page</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=liuaishan&repo=SpatiotemporalAttack&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
				
                 <p align="justify" style="font-size:13px">We take the first step to study adversarial attacks for embodied agents. </p>
                <p></p>
            </td>
        </tr>
		
   <td width="20%"><img src="./imgs/ECCV_2.png" alt="eccv2020" width="180" height = "110" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://arxiv.org/pdf/2005.09257.pdf">
                 <papertitle>Bias-based Universal Adversarial Patch Attack for Automatic Check-out</papertitle></a>
                 <br><strong>Aishan Liu</strong>, Jiakai Wang, Xianglong Liu, Bowen Cao, Chongzhi Zhang, Hang Yu.<br>
                 <em>European Conference on Computer Vision (ECCV)</em>, 2020
                 <br>
                 <a href="https://arxiv.org/pdf/2005.09257.pdf">pdf</a> /
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652073635&idx=5&sn=b1acd091996cacb9e74053c4208b793c&chksm=f1201a52c6579344ae75ccb2ee3042ed3eddbb6bd988000c7b1ba8c274f1aceaec7ea1300d1d&mpshare=1&scene=1&srcid=07082kvhcWURQF4VRcIx8uU9&sharer_sharetime=1596855924325&sharer_shareid=da9c9379a79901c18dc93793609d62fa&key=4defdd0e8978fadbc4f7d3467b572eb060d5482035de4befee54b935c6aabbcaafa3ed60343840f82abb27fbcc57798b93e6f215c44f11a37e87141722c58b1167ffeba220d7150c5c8ee9333d06f513&ascene=1&uin=MTQzMTA0NDAw&devicetype=Windows+10+x64&version=62090529&lang=zh_CN&exportkey=AfGxCG3P9erzoZeJcwLMjbg%3D&pass_ticket=qME0ljezjearOlDwgFgo%2F6ZH0VZ%2B7CLScg%2FNCc5rqMk%3D"><font color="red">(新智元)</font></a>
                /<a href="https://github.com/liuaishan/ModelBiasedAttack"><font color="red">Project page</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=liuaishan&repo=ModelBiasedAttack&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
				
                 <p align="justify" style="font-size:13px">We propose a bias-based framework to generate class-agnostic universal adversarial patches with strong generalization ability, which exploits both the perceptual and semantic bias of models.  </p>
                <p></p>
            </td>
        </tr>


        <td width="20%"><img src="./imgs/IJCAI2020_1.png" alt="ijcai2019" width="180" height = "110" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://www.ijcai.org/Proceedings/2020/0113.pdf">
                 <papertitle>Few-shot Visual Learning with Contextual Memory and Fine-grained Calibration</papertitle></a>
                 <br>Yuqing Ma, Shihao Bai, Wei Liu, Qingyu Zhang, <strong> Aishan Liu </strong>, Weimin Chen, Xianglong Liu<br>
                 <em>International Joint Conference on Artificial Intelligence (IJCAI)</em>, 2020
                 <br>
                 <a href="https://www.ijcai.org/Proceedings/2020/0113.pdf">pdf</a>
                 <p align="justify" style="font-size:13px">To improve the generalization ability of few-shot learning, we propose an inverted pyramid network that intimates the human’s coarse-to-fine cognition paradigm.</p>
                <p></p>
            </td>
        </tr>
		
		

        <td width="20%"><img src="./imgs/IJCAI2020_2.png" alt="ijcai2020" width="180" height="110" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://www.ijcai.org/Proceedings/2020/0112.pdf">
                 <papertitle>Transductive Relation-Propagation Network for Few-shot Learning</papertitle></a>
                 <br>Yuqing Ma, Shihao Bai, Shan An, Wei Liu, <strong>Aishan Liu</strong>, Xiantong Zhen, Xianglong Liu.<br>
                 <em>International Joint Conference on Artificial Intelligence (IJCAI)</em>, 2020
                 <br>
                 <a href="https://www.ijcai.org/Proceedings/2020/0112.pdf">pdf</a> /
				 <a href="https://github.com/vickyFox/TRPN"><font color="red">Project page</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=vickyFox&repo=TRPN&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
				
                 <p align="justify" style="font-size:13px">For few-shot learning task, we propose a transductive relation-propagation graph neural network to explicitly model and propagate such relations across support-query pairs. </p>
                <p></p>
            </td>
        </tr>
		
        <td width="20%"><img src="./imgs/ijcai19.jpg" alt="ijcai2019" width="180" height = "110" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://www.ijcai.org/Proceedings/2019/433">
                 <papertitle>Coarse-to-Fine Image Inpainting via Region-wise Convolutions and Non-Local Correlation</papertitle></a>
                 <br>Yuqing Ma, Xianglong Liu, Shihao Bai, Lei Wang, Dailan He, <strong> Aishan Liu </strong><br>
                 <em>International Joint Conference on Artificial Intelligence (IJCAI)</em>, 2019
                 <br>
                 <a href="https://www.ijcai.org/Proceedings/2019/433">pdf</a>
                 <p align="justify" style="font-size:13px"> To address the image inpainting problem, we propose a coarse-to-fine framework to restore semantically reasonable and visually realistic images, which consists region-wise convolutions to locally deal with the different types of regions and non-local operations to globally model the correlation among different regions.</p>
                <p></p>
            </td>
        </tr>

        <td width="20%"><img src="./imgs/aaai19.png" alt="aaai2019" width="180" height = "110" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://ojs.aaai.org//index.php/AAAI/article/view/3893">
                 <papertitle>Perceptual Sensitive GAN for Generating Adversarial Patches</papertitle></a>
                 <br><strong>Aishan Liu</strong>, Xianglong Liu, Jiaxin Fan, Yuqing Ma, Anlan Zhang, Huiyuan Xie and Dacheng Tao.<br>
                 <em>AAAI Conference on Artificial Intelligence (AAAI)</em>, 2019
				 <font color="red"><strong>(Spotlight)</strong></font>
                 <br>
                 <a href="https://ojs.aaai.org//index.php/AAAI/article/view/3893">pdf</a> /
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652044274&idx=3&sn=5e2570c25133c03e30930fdc9b04f2f8&chksm=f1218f03c65606157a07db05cad07264a38171038a8de33b667f21af001c76d3baaa501a7506&mpshare=1&scene=1&srcid=08085db4ycxZdHGWEPBNfGv7&sharer_sharetime=1596855953802&sharer_shareid=da9c9379a79901c18dc93793609d62fa&key=4bdaf1520bf406e77cac6eeb1e04025fad0a6dfcc38554ec2ae8223439dee1b518b3e5e70aa6d4335a271be601450da62f8d4466fe85fc3717b9650521117d32b52c3d3727bc25459b0c6e722caad691&ascene=1&uin=MTQzMTA0NDAw&devicetype=Windows+10+x64&version=62090529&lang=zh_CN&exportkey=ARP19W6ltvF%2B%2B%2BBJE5uc728%3D&pass_ticket=qME0ljezjearOlDwgFgo%2F6ZH0VZ%2B7CLScg%2FNCc5rqMk%3D"><font color="red">(新智元,</font></a>
                <a href="https://cloud.tencent.com/developer/article/1425611"><font color="red">腾讯,</font></a>
                <a href="https://www.163.com/dy/article/EEGALRRB0511ABV6.html"><font color="red">网易)</font></a>
                 <p align="justify" style="font-size:13px">We propose a PS-GAN framework to generate adversarial patches to attack auto-driving systems in the physical world. </p>
                <p></p>
            </td>
        </tr>
		</tbody>
		</table>
		
		<h3>&nbsp;&nbsp;&nbsp;&nbsp;Journal Papers</h3>
		
		
		  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody><tr>

		<td width="20%"><img src="./imgs/SNS.png" alt="TIP2021" width="180" height = "110" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://arxiv.org/pdf/1909.06978.pdf">
                 <papertitle>Interpreting and Improving Adversarial Robustness of Deep Neural Networks with Neuron Sensitivity</papertitle></a>
                 <br>Chongzhi Zhang*, <strong>Aishan Liu*</strong>, Xianglong Liu, Yitao Xu, Hang Yu, Yuqing Ma, Tianlin Li (* indicates equal contributions)<br>
                 <em>IEEE Transactions on Image Processing (TIP)</em>, 2021
				 <font color="red"><strong>(IF=9.34)</strong></font>
                 <br>
                 <a href="https://arxiv.org/pdf/1909.06978.pdf">pdf</a> /
				 <a href="https://github.com/DIG-Beihang/AISafety"><font color="red">Project page</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=DIG-Beihang&repo=AISafety&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
				
                 <p align="justify" style="font-size:13px">We are the first to explain adversarial robustness for deep models from the perspective of neuron sensitivity, which is measured by neuron behavior variation intensity against benign and adversarial examples.</p>
                <p></p>
            </td>
        </tr>
		
		<td width="20%"><img src="./imgs/INS_1.png" alt="TIP2021" width="180" height = "110" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://doi.org/10.1016/j.ins.2020.08.043">
                 <papertitle>Understanding Adversarial Robustness via Critical Attacking Route</papertitle></a>
                 <br>Tianlin Li*, <strong>Aishan Liu*</strong>, Xianglong Liu, Yitao Xu, Chongzhi Zhang, Xiaofei Xie (* indicates equal contributions)<br>
                 <em>Information Sciences</em>, 2020
				 <font color="red"><strong>(IF=5.91)</strong></font>
                 <br>
                 <a href="https://doi.org/10.1016/j.ins.2020.08.043">pdf</a> /
				 <a href="https://github.com/DIG-Beihang/AISafety"><font color="red">Project page</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=DIG-Beihang&repo=AISafety&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
				
                 <p align="justify" style="font-size:13px">We try to explain adversarial robustness for deep models from a new perspective of critical attacking route, which is computed by a gradient-based influence propagation strategy.</p>
                <p></p>
            </td>
        </tr>
		
		<td width="20%"><img src="./imgs/AI-View.png" alt="AIView" width="180" height = "110" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="http://www.cesi.ac.cn/202007/6566.html">
                 <papertitle>人工智能安全与评测</papertitle></a>
                 <br> <strong>刘艾杉</strong>, 王嘉凯, 刘祥龙<br>
                 <em>人工智能(AI-View)</em>, 2020
				 
                 <br>
                 <a href="http://www.cesi.ac.cn/202007/6566.html">pdf</a> 
                 
                <p></p>
            </td>
        </tr>
	
			<td width="20%"><img src="./imgs/Eval_1.png" alt="AIView" width="180" height = "110" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="http://www.cesi.ac.cn/202007/6566.html">
                 <papertitle>人工智能机器学习模型及系统的质量要素和测试方法</papertitle></a>
                 <br> 王嘉凯, <strong>刘艾杉</strong>, 刘祥龙<br>
                 <em>信息技术与标准化</em>, 2020
				 
                 <br>
                 <a href="http://www.cesi.ac.cn/202007/6566.html">pdf</a> 
                 
                <p></p>
            </td>
        </tr>
	



        </tbody>
    </table>


    <!--Thesis  -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
        <tbody><tr>
          <td><heading>Highlight Project</heading>
          </tr></tbody>
    </table>
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody><tr>
        <td width="20%"><img src="./imgs/logo设计-02.png" alt="PontTuset" width="180" height="160" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://arxiv.org/pdf/2101.09617.pdf">
                 <papertitle>重明 (AISafety)</papertitle></a>
                 <br>
                  <br>
                 <a href="https://arxiv.org/pdf/2101.09617.pdf">pdf</a> /
				  <a href="http://www.techweb.com.cn/2020-12-02/2814466.shtml"><font color="red"> (News: TechWeb)</font></a> /
				 <a href="https://github.com/DIG-Beihang/AISafety"><font color="red">Project page</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=DIG-Beihang&repo=AISafety&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
				
                
                 <p align="justify" style="font-size:13px">重明 is an open-source platform to evaluate model robustness and safety towards noises (e.g., adversarial examples, corruptions, etc.). 
				 The name is taken from the Chinese myth <a href="https://baike.baidu.com/item/%E9%87%8D%E6%98%8E%E9%B8%9F/5482222?fr=aladdin">重明鸟</a>, which has strong power, could fight against beasts and avoid disasters. 
				 We hope our platform could improve the robustness of deep learning systems and help them to avoid safety-related problems. 
				 重明 has been awarded the <a href="http://www.techweb.com.cn/2020-12-02/2814466.shtml">首届OpenI启智社区优秀开源项目</a> (First OpenI Excellent Open Source Project).
                 </p>
                <p></p>
            </td>
        </tr>

        </tbody>
    </table>

    <!--SECTION 6 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><heading>Talks & Academic Services</heading>
			 <p style="font-size:13px"> <strong>[2021.03]</strong> Co-organizer of the Workshop on <a href="https://advm-workshop-2021.github.io/">1st International Workshop on Adversarial Learning for Multimedia</a> at ACM MM 2021. </p>
             <p style="font-size:13px"> <strong>[2020.12]</strong> Invited talk about Adversarial Machine Learning at <a href="https://zhidx.com/p/246975.html">智东西公开课(Zhidx)</a></p>
             <p style="font-size:13px"> <strong>[2020.11]</strong> Invited talk about Adversarial Attacks for Embodiment at <a href="http://www.csai.tsinghua.edu.cn/">CSAI, Tsinghua University</a>, hosted by <a href="http://www.cs.tsinghua.edu.cn/publish/cs/4616/2013/20131122152618871262231/20131122152618871262231_.html">Prof. Huaping Liu</a>.</p>
             <p style="font-size:13px"> <strong>[2020.08]</strong> Invited talk about AI Safety in Automatic Check-out Scenario at <a href="https://baijiahao.baidu.com/s?id=1675149234904040117&wfr=spider&for=pc">智东西公开课(Zhidx)</a></p>
             <p style="font-size:13px"> <strong>[2020.07]</strong> Invited talk about Adversarial Machine Learning in Physical World at JD.</p>
            </td>
            </tr></tbody>
    </table>


    <!--SECTION 7 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><heading>Main Awards</heading>
			 <p style="font-size:13px"> <strong>[2020.12]</strong> First OpenI Excellent Open Source Project (Nationwide 7)</p>
             <p style="font-size:13px"> <strong>[2019.05]</strong> Tencent Rhino-Bird Elite Training Program (Nationwide 56)</p>
			 <p style="font-size:13px"> <strong>[2016.01]</strong> Outstanding Graduate Award, Beihang University</p>
			 <p style="font-size:13px"> <strong>[2013.06]</strong> Outstanding Graduate Award, Beijing</p>
			 <p style="font-size:13px"> <strong>[2012.10]</strong> CCF National Outstanding Undergraduate, China Computer Federation (Nationwide 100)</p>
			 <p style="font-size:13px"> <strong>[2012.06]</strong> Google Excellence Scholarship, Google (Nationwide 100)</p>
            </td>
            </tr></tbody>
    </table>

 


    <!--SECTION 9 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
    <td width="100%" align="middle">
    <p align="center" style="width: 25% ">
	<script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=qq8MT_oSoUYk02rXMCrx61IGe3LXvDIHSIgP2ou8k48"></script>
    </p></td>
    </tr>
    </tbody>
    </table>


    <!--SECTION 10 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><br>
               <!--<p align="right"><font size="3">Erd&ouml;s = ? </font><br> -->
		       <p align="right"><font size="2"> Last update: 2021.03</font></p>
            </td>
         </tr>
         </tbody>
     </table>


</td>
</tr>
</tbody>
</table>
</body>
</html>
